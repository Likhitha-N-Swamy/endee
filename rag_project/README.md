# RAG-based AI Assistant using Endee Vector Database

A Retrieval Augmented Generation (RAG) application that answers questions over your documents using the **Endee** vector database for fast similarity search, with a FastAPI service and optional CLI.

---

## 1. Overview

This project implements a full RAG pipeline:

- **Ingest** documents (e.g. from `data/sample_docs.txt`), chunk them, embed with **sentence-transformers** (`all-MiniLM-L6-v2`), and store vectors in **Endee**.
- **Retrieve** the most relevant chunks for a user question via Endee’s search API and local metadata.
- **Answer** by building a context + question prompt and passing it to an answer generator (placeholder by default; easily swapped for a real LLM).

You can run retrieval from the command line or via a **FastAPI** app that exposes a simple `/ask` endpoint.

---

## 2. Why Endee

**Endee** is used as the vector database for several reasons:

- **Performance** – Built for low-latency similarity search using HNSW and optimized storage (e.g. MDBX), with optional quantization (e.g. INT8) to reduce memory and improve throughput.
- **REST API** – Index creation, vector insert, and search are exposed over HTTP, so the Python stack stays simple (no native DB driver); we use `requests` for all calls.
- **Open source** – Apache 2.0; you can run it on-prem or in your own Docker setup without vendor lock-in.
- **Operational simplicity** – Run Endee via Docker (or the official image from Docker Hub) and point this project at `http://localhost:8080` (or your deployed URL).

Together, this makes Endee a good fit for a recruiter-friendly, demo-ready RAG assistant that clearly shows vector DB integration.

---

## 3. Architecture

End-to-end RAG workflow:

1. **Index creation** – Create an Endee index (e.g. `rag_index`) with dimension 384 and cosine space to match the embedding model.
2. **Ingestion** – Read source text → split into small chunks (e.g. 2–3 sentences) → embed each chunk with sentence-transformers → POST vectors to Endee with unique IDs → save chunk text in `data/chunk_metadata.json` for later lookup.
3. **Retrieval** – User asks a question → embed the question with the same model → POST the vector to Endee’s search API (top-k) → decode MessagePack response → map result IDs to original text using `chunk_metadata.json` → return ordered list of chunks.
4. **Answer generation** – Concatenate retrieved chunks into a single context string → build prompt `Context:\n{context}\n\nQuestion:\n{question}\n\nAnswer:` → call answer generator (placeholder or real LLM) → return the final answer string.

The FastAPI app wraps steps 3–4: it accepts a `question`, calls `run_rag(question)`, and returns `{ "question": "...", "answer": "..." }`.

---

## 4. Project Structure

| Path | Purpose |
|------|--------|
| `app.py` | FastAPI app: root `/` and GET `/ask?question=...`; calls `run_rag()`. |
| `rag_pipeline.py` | RAG orchestration: `run_rag(question)` → retrieve chunks, build prompt, call placeholder LLM, return answer. |
| `ingestion/ingest.py` | Reads `data/sample_docs.txt`, chunks, embeds, inserts vectors into Endee, writes `data/chunk_metadata.json`. |
| `retrieval/search.py` | `retrieve_chunks(query)` → embed query, search Endee, map IDs to text using `chunk_metadata.json`. |
| `utils/create_index.py` | Creates the Endee index `rag_index` (dim 384, cosine) via POST to `/api/v1/index/create`. |
| `data/sample_docs.txt` | Sample source document. |
| `data/chunk_metadata.json` | Generated by ingest; maps chunk IDs to original text for retrieval. |
| `requirements.txt` | Python dependencies (sentence-transformers, requests, fastapi, uvicorn, msgpack). |

---

## 5. Setup Instructions

### Endee (Docker)

Run Endee so the REST API is available at `http://localhost:8080`:

```bash
# From the Endee repo root (or use the pre-built image)
docker run -p 8080:8080 -v endee-data:/data --name endee-server endeeio/endee-server:latest
```

Leave `NDD_AUTH_TOKEN` unset for no auth, or set it and add the token to the scripts/API if you use auth.

### Python

From the **rag_project** directory:

```bash
cd rag_project
python -m venv .venv
.venv\Scripts\activate   # Windows
# source .venv/bin/activate   # Linux / macOS
pip install -r requirements.txt
```

---

## 6. How to Run

All commands below assume you are in **rag_project** and the virtualenv is activated. Endee must be running on port 8080.

| Step | Command | Description |
|------|---------|-------------|
| 1. Create index | `python utils/create_index.py` | Creates `rag_index` (384 dim, cosine). |
| 2. Ingest docs | `python -m ingestion.ingest` | Chunks `data/sample_docs.txt`, embeds, inserts into Endee, writes `data/chunk_metadata.json`. |
| 3. Search (CLI) | `python -m retrieval.search "What is RAG?"` | Retrieves top chunks and prints them. |
| 4. RAG (CLI) | `python rag_pipeline.py "What is RAG?"` | Runs full RAG and prints the answer. |
| 5. API | `uvicorn app:app --reload` | Start FastAPI; use GET `/ask?question=...` for RAG answers. |

---

## 7. Example Usage

**CLI (RAG pipeline):**

```bash
python rag_pipeline.py "What is Endee?"
```

**Example output (placeholder LLM):**

```
Based on the context: Endee is a high-performance open-source vector database. In answer to "What is Endee?": The relevant information is in the context above.
```

**API:**

```bash
curl "http://localhost:8000/ask?question=What%20is%20RAG?"
```

**Example response:**

```json
{
  "question": "What is RAG?",
  "answer": "Based on the context: Retrieval Augmented Generation combines vector search with LLMs. In answer to \"What is RAG?\": The relevant information is in the context above."
}
```

---

## 8. Evaluation Checklist

| Requirement | How this project addresses it |
|-------------|-------------------------------|
| Use Endee as the vector database | All vectors are stored and queried via Endee’s REST API (index create, vector insert, search). |
| Ingestion pipeline | `ingestion/ingest.py` reads source text, chunks, embeds with sentence-transformers, and inserts into Endee with stable IDs; chunk text is stored in `chunk_metadata.json`. |
| Retrieval | `retrieval/search.py` embeds the query, calls Endee search, and maps result IDs back to text using `chunk_metadata.json`. |
| RAG workflow | `rag_pipeline.py` implements retrieve → context → prompt → answer; prompt format is `Context:\n{context}\n\nQuestion:\n{question}\n\nAnswer:`. |
| API / interface | FastAPI app in `app.py` exposes GET `/ask?question=...` and returns JSON `{ "question", "answer" }`; root `/` describes usage. |
| Documentation | This README covers overview, architecture, setup, run steps, example usage, and extension points. |
| Reproducibility | Setup (Docker + Python) and run order (create index → ingest → search/API) are clearly documented. |

---

## 9. Notes

- **Placeholder LLM** – The current answer is produced by a rule-based placeholder in `rag_pipeline.py` (`placeholder_llm()`). It does not call any external LLM. To use a real model, replace the body of `placeholder_llm(prompt)` with a call to your chosen API (e.g. OpenAI, Anthropic, or a local model), still taking the same prompt string and returning a single answer string.
- **Extending the pipeline** – You can plug in different chunking, embedding models, or LLMs as long as the index dimension and space type match (e.g. 384 and cosine for `all-MiniLM-L6-v2`). Update `utils/create_index.py` and the embedding calls in `ingestion/ingest.py` and `retrieval/search.py` if you change the model.
- **Auth** – If Endee is run with `NDD_AUTH_TOKEN`, set `AUTH_TOKEN` in the relevant scripts (`utils/create_index.py`, `ingestion/ingest.py`, `retrieval/search.py`) and pass it in the `Authorization` header.
